exp:
    id: mtmaunet ## expriment ID,
    output_dir: data ## 
    accelerator:
        fp16: True  # amp train
        cpu: False  # cuda  or  cpu

dataset:
    root: ./your_data_dir ## input root path, containing images dir and label dir. Here also locate the data split dir 
    cv:   # csv fold k dir
        dir_name: cv
        num: 5
        fold: 0 
        shuffle: True
        random_state: 0

    split: ## data split
        train: train.csv   # train csv data name
        val: val.csv  # val csv data name
    test: ./test.csv   # infer csv path

    dim: &dim 3  ## data dimension, for example, dim=3 when your data is CT data in .mha format 
    channel: &channel 1  ## data channel, usually 1 for CT or MR, and 3 for natural 2D image. 
    n_classes: &n_classes 2  ## number of classes 
    patch_size:  &patch_size [320,320,24]    #   [H,W,[D]]
    no_channel: True  # have  Channel

dataloader:
    num_workers: 16  # set to 0 during inference!!! 

network: # network name and its parameters when defination needed 
    type: mtmaunet
    unet:
        name: unet
        spatial_dims: *dim
        in_channels: *channel
        out_channels: *n_classes
        channels: [16, 32, 64, 128, 256]
        strides: [2, 2, 2, 2]
        num_res_units: 2
    mtmaunet:
        name: mtmaunet
        spatial_dims: *dim
        in_channels: *channel
        out_channels: *n_classes
        kernel_size: [[3,3,1],[3,3,1],[3,3,1],[3,3,3], [3,3,3]]
        strides:  [[1,1,1],[2,2,1], [2,2,1], [2,2,1], [2,2,2]]
        upsample_kernel_size: [[2,2,1], [2,2,1], [2,2,1], [2,2,2]]
        filters: [16, 32, 64, 128, 256]
        dropout: 0.2
        deep_supervision: False
        deep_supr_num: 1
        res_block:  True
        trans_bias: False
        trans_dim: 512
        class_num: 2

init_model: None ## initial pre-trained model path 
val_model: /path/of/your/checkpoint


solver:
    ft: False
    loss:   #  loss defination 
        dicece:
            self_weight: 1
            include_background: True
            to_onehot_y: True
            sigmoid: False
            softmax: True
            lambda_dice: 1.0
            lambda_ce: 1.0
    batch_size: 
        train: 4
        test: 1
    sw_batch_size: 2
    iter_tb: 1
    optimizer:  ## optimizer name and its parameters when defination needed 优化器和定义时传入的形参
        ## adam
        name: adam
        weight_decay: 1.0e-5
        lr: 1.0e-4
        
    epoch_save: 5
    epoch_start: 0
    epoch_max: 300
    lr_scheduler: cos # 'poly','step','cos' # learning change scheduler defination
    warmup_epochs: 10


    